{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1EcXa6y426V"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwAdYd3Bj2k"
      },
      "source": [
        "Create a new neuron network(a set of layers)\r\n",
        "then add convolutional layers.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNmIbu4RSsEq"
      },
      "source": [
        "This link provides a detailed description on what convolutional layers do. https://cs231n.github.io/convolutional-networks/#conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-suFll9DTFAK"
      },
      "source": [
        "Basically a convolutional layer takes in a 3d data: length * width * 3\r\n",
        "(RGB), and then use a 3d array(called filter, has smaller dimension in length and width than input, but have a depth of 3) to slide through our input and take the dot product between it and the region it's on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zAn1ahOWacB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H6KrsD7T7py"
      },
      "source": [
        "Usually, mutiple different filters are used to slide through input array.\r\n",
        "Each filter provides a 2d array as it slides through input array and takes \r\n",
        "dot product. So, if n different filters are used, we'll have n different resulting 2d arrays. We stack them up to get a 3d array as our output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSe2LYcxVT6p"
      },
      "source": [
        "tf.keras.layers.Conc2D: first integer specifies how many filters we want to use. The second argument, a tuple, specifies the size of each filter. Conv2D can take in a lot more arguments, but I haven't studied all of them. Here's a link to the documentation of Conv2D:https://keras.io/api/layers/convolution_layers/convolution2d/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7cmdCV3WUFZ"
      },
      "source": [
        "I think we can add as many layers as we want and use filters of any shape, so I just randomly created 3 layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBWVVFrz426b"
      },
      "source": [
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.layers.Conv2D(10, (3, 3), input_shape=(32, 32, 3)))\r\n",
        "model.add(tf.keras.layers.Conv2D(10, (3, 3)))\r\n",
        "model.add(tf.keras.layers.Conv2D(10, (3, 3)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2roRg088WSYJ"
      },
      "source": [
        "Since we need to do image classfication, we want a 1d array which represents\r\n",
        "the probablity that our image belong to a certain class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROR7yUZHYL0i"
      },
      "source": [
        "Fully connected layers does that. They are 1d layers that produce another 1d layer, which is our desired output.\r\n",
        " A detailed description on fully connected layers: https://cs231n.github.io/convolutional-networks/#fc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5C8BNq5Y8LS"
      },
      "source": [
        "tf.keras.layers.Flatten() converts our input data(currently 3d) to a 1d array that fully connected layers need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ2G-a-jZNy4"
      },
      "source": [
        "tf.keras.layers.Dense() creates a fully connected layer. It can take an integer argument that specifies the size of its output 1d array. It can also take a lot other arguments that I haven't explored yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-RFKnx8ZlnK"
      },
      "source": [
        "We can stack as many fully connected layers as we want, so I just made two.\r\n",
        " But note that the size of output of the last layer should be 3 since the image we want to classify belong to 3 classes: not wearing a mask, wearing a mask, and wearing mask improperly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9kzE50IA-Gy"
      },
      "source": [
        "model.add(tf.keras.layers.Flatten())\r\n",
        "model.add(tf.keras.layers.Dense(64))\r\n",
        "model.add(tf.keras.layers.Dense(3))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVitPYF4apAJ"
      },
      "source": [
        "Next we need to load our dataset. This is a useful link:https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMJuWBpeNhx"
      },
      "source": [
        "I'm just copying and pasting the code from the previous webpage since tje \r\n",
        "code will work. One importance thing is putting all images into the right directory. Specify dataset directory in the first arguement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "xmlPSE4cBgNU",
        "outputId": "24d25c82-e4a0-4e5a-8f40-e6c63c05f466"
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\r\n",
        "train_it = datagen.flow_from_directory('data/train/', class_mode='categorical', batch_size=64)\r\n",
        "val_it = datagen.flow_from_directory('data/validation/', class_mode='categorical', batch_size=64)\r\n",
        "test_it = datagen.flow_from_directory('data/test/', class_mode='categorical', batch_size=64)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-8701639e0d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/validation/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2NqlxeWe9Gp"
      },
      "source": [
        "We can train our model after loading dataset. Evaluate our model use evaluate_generator method, and predict our result with the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfu5x5mXfFiW"
      },
      "source": [
        "model.fit_generator(train_it, steps_per_epoch=16, validation_data=val_it, validation_steps=8)\r\n",
        "loss = model.evaluate_generator(test_it, steps=24)\r\n",
        "yhat = model.predict_generator(predict_it, steps=24)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}